{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "pjbx2j6HrOI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-rQ0WatCD9C",
        "outputId": "2149224a-fdd0-4797-d16d-eaa822e1428a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.12.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.106.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "%cd\n",
        "%cd /content/\n",
        "\n",
        "! pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/orpatashnik/StyleCLIP\n",
        "!git clone https://github.com/esoyeon/Multilingual-StyleCLIP.git\n",
        "\n",
        "# multilingual set up\n",
        "!pip install ftfy regex tqdm gdown\n",
        "!pip install transformers\n",
        "\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaB_ltg8q34H",
        "outputId": "df9ee2dd-d8ec-4056-a077-cd4dc2251580"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.1+cu110 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.1+cu110\u001b[0m\u001b[31m\n",
            "\u001b[0mfatal: destination path 'encoder4editing' already exists and is not an empty directory.\n",
            "--2023-12-26 17:57:07--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231226T175707Z&X-Amz-Expires=300&X-Amz-Signature=088142e1fe93461581a828bb3dd3f20fcef0990c12c9633421a48704d44f320b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-12-26 17:57:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231226T175707Z&X-Amz-Expires=300&X-Amz-Signature=088142e1fe93461581a828bb3dd3f20fcef0990c12c9633421a48704d44f320b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.1’\n",
            "\n",
            "ninja-linux.zip.1   100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-12-26 17:57:07 (6.27 MB/s) - ‘ninja-linux.zip.1’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "/root\n",
            "/content\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bxq3hb_4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bxq3hb_4\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "fatal: destination path 'StyleCLIP' already exists and is not an empty directory.\n",
            "fatal: destination path 'Multilingual-StyleCLIP' already exists and is not an empty directory.\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e4e setup\n",
        "\n",
        "from gdown import download as drive_download\n",
        "drive_download(\"https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\", \"/content/encoder4editing/e4e_ffhq_encode.pt\", quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "os.chdir('/content/encoder4editing')\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": \"e4e_ffhq_encode.pt\"\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK6DugkCrNW2",
        "outputId": "d83e8189-bd47-4804-bd6b-5392c0434e25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\n",
            "To: /content/encoder4editing/e4e_ffhq_encode.pt\n",
            "100%|██████████| 1.20G/1.20G [00:06<00:00, 189MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: e4e_ffhq_encode.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Alignment, Inversion"
      ],
      "metadata": {
        "id": "J15vYkxCrQSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alignment\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from utils.alignment import align_face\n",
        "\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "\n",
        "    return aligned_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB4Yy8EHrUU_",
        "outputId": "f7e3fe81-173e-49e3-cab2-0c4936404e99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-26 17:57:53--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2.1’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  98.0MB/s    in 0.6s    \n",
            "\n",
            "2023-12-26 17:57:53 (98.0 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2.1’ saved [64040097/64040097]\n",
            "\n",
            "bzip2: Output file shape_predictor_68_face_landmarks.dat already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inversion\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents"
      ],
      "metadata": {
        "id": "fBIlG8Zerc7e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Direction"
      ],
      "metadata": {
        "id": "nbiFAw1erdvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "os.chdir('/content/Multilingual-StyleCLIP')\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# multilingual clip weights\n",
        "!bash get-weights.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKer0WRKrhJS",
        "outputId": "02e8b930-bde2-40b5-dfc0-9f61d0a769aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-26 17:57:54--  https://www.dropbox.com/s/2oxu7hw0y9fwdqs/M-BERT-Base-69-ViT%20Linear%20Weights.pkl\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/2oxu7hw0y9fwdqs/M-BERT-Base-69-ViT%20Linear%20Weights.pkl [following]\n",
            "--2023-12-26 17:57:55--  https://www.dropbox.com/s/raw/2oxu7hw0y9fwdqs/M-BERT-Base-69-ViT%20Linear%20Weights.pkl\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com/cd/0/inline/CKJ4BIGEbZ3bD6NCzp4kOhyx2v1Dxwp8P4y6VxKfmmsSTJWqZg3XTY1LOtgsLONQkC2V_ChVvsm5fqwOYgn0EwM0pxGe9jYxb7pOYlzI4JJlg1JiKQAkFyGs_0HouPOoT60Z33iHGpGaxw-Tw--2LIYi/file# [following]\n",
            "--2023-12-26 17:57:55--  https://uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com/cd/0/inline/CKJ4BIGEbZ3bD6NCzp4kOhyx2v1Dxwp8P4y6VxKfmmsSTJWqZg3XTY1LOtgsLONQkC2V_ChVvsm5fqwOYgn0EwM0pxGe9jYxb7pOYlzI4JJlg1JiKQAkFyGs_0HouPOoT60Z33iHGpGaxw-Tw--2LIYi/file\n",
            "Resolving uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com (uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com (uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CKLCzcD6MmOGRmJSqFMy6DKEORsVLVdjd1YDohaVW56uCGmJawjYEZGpHFpYyc18ZbzyLMUhn0ToXFdwiqIoVF7j-BMFuYAIdzw2dA-JEwC_vjiENIxRZnVCZl5fvksn7jkGhn6HJQO4DaXaEuRJ38g-jAZFWXuLzW8CM0GrU22rlK45SnXxkQFlcspx4l3EFMvdew1HkM7dcVwoxwt1mpIzoLDthixX3XAQQxHZxLBJ-ohjxXDRFh8qk-ic3KEMMfiI7GELYvrszLm7QR1RLTB2UHNU3n3xEXpRz8v-28-7xaT5mG4EsSpDVfBk-CnoXRIXXonHydoA3x0Bc6dNMcc0UTjV9sEmYH4Panws_8p_Ptr8RlOeNLN0Ncdk-mbOJug/file [following]\n",
            "--2023-12-26 17:57:56--  https://uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com/cd/0/inline2/CKLCzcD6MmOGRmJSqFMy6DKEORsVLVdjd1YDohaVW56uCGmJawjYEZGpHFpYyc18ZbzyLMUhn0ToXFdwiqIoVF7j-BMFuYAIdzw2dA-JEwC_vjiENIxRZnVCZl5fvksn7jkGhn6HJQO4DaXaEuRJ38g-jAZFWXuLzW8CM0GrU22rlK45SnXxkQFlcspx4l3EFMvdew1HkM7dcVwoxwt1mpIzoLDthixX3XAQQxHZxLBJ-ohjxXDRFh8qk-ic3KEMMfiI7GELYvrszLm7QR1RLTB2UHNU3n3xEXpRz8v-28-7xaT5mG4EsSpDVfBk-CnoXRIXXonHydoA3x0Bc6dNMcc0UTjV9sEmYH4Panws_8p_Ptr8RlOeNLN0Ncdk-mbOJug/file\n",
            "Reusing existing connection to uc63933eddaa2afb854f757bad21.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text prompt\n",
        "\n",
        "imagenet_templates = [\n",
        "'{}의 나쁜 사진.',\n",
        "'{}의 조각상.',\n",
        "'{} 보기 어려운 사진.',\n",
        "'{}의 저해상도 사진.',\n",
        "'{}의 렌더링.',\n",
        "'{}의 숫자.',\n",
        "'나쁜 {} 사진.',\n",
        "'{}를 자른 사진.',\n",
        "'{}의 문신.',\n",
        "'자수 {}.',\n",
        "'{} 보기 힘든 사진.',\n",
        "'{}의 밝은 사진.',\n",
        "'깨끗한 {}의 사진.',\n",
        "'더러운 {}의 사진.',\n",
        "'{}의 어두운 사진.',\n",
        "'{} 그림.',\n",
        "'내 {}의 사진.',\n",
        "'플라스틱 {}',\n",
        "'멋진 {}의 사진.',\n",
        "'{}의 클로즈업 사진.',\n",
        "'{}의 흑백 사진.',\n",
        "'{}을 그린 그림.',\n",
        "'{}를 그린 그림.',\n",
        "'{}의 픽셀화된 사진.',\n",
        "'{}의 조각상.',\n",
        "'{}의 밝은 사진.',\n",
        "'{}의 자른 사진.',\n",
        "'플라스틱 {}.' ,\n",
        "'더러운 {}의 사진.',\n",
        "'{} JPEG 손상 사진.',\n",
        "'{} 흐릿한 사진.',\n",
        "'{} 사진.',\n",
        "'{} 좋은 사진.',\n",
        "'{} 렌더링.',\n",
        "'비디오 게임 {}.',\n",
        "'한 {} 사진.',\n",
        "'{} 낙서.',\n",
        "'{} 클로즈업 사진.',\n",
        "'{} 사진.',\n",
        "'종이접기 {}.',\n",
        "'{} 비디오 게임.',\n",
        "'{}의 스케치.',\n",
        "'{}의 낙서.',\n",
        "'{} 종이접기.',\n",
        "'{}의 저해상도 사진.',\n",
        "'장난감 {}.' ,\n",
        "'{}을 변형한 것.',\n",
        "'청렴한 {}의 사진.',\n",
        "'큰 {}의 사진.',\n",
        "'{}을 해석한 것.',\n",
        "'멋진 {}의 사진.',\n",
        "'이상한 {}의 사진.',\n",
        "'{}의 흐릿한 사진.',\n",
        "'만화 {}.' ,\n",
        "'{} 작품.' ,\n",
        "'{}의 스케치.',\n",
        "'{} 자수.',\n",
        "'{} 픽셀화된 사진.',\n",
        "'{} itap.',\n",
        "'{} JPEG 손상 사진.',\n",
        "'좋은 {}의 사진.',\n",
        "'{} 인형.' ,\n",
        "'멋진 {}의 사진.',\n",
        "'작은 {}의 사진.',\n",
        "'이상한 {}의 사진.',\n",
        "'만화 {}.' ,\n",
        "'{} 예술.',\n",
        "'{}을 그립니다.',\n",
        "'큰 {}의 사진.',\n",
        "'흑백 {} 사진.',\n",
        "'인형 {}.' ,\n",
        "'어두운 {} 사진.',\n",
        "'내가 찍은 {} 사진.',\n",
        "'{} 중 하나.',\n",
        "'장난감 {}.' ,\n",
        "'내 {} itap.',\n",
        "'멋진 {} 사진.',\n",
        "'작은 {}의 사진.',\n",
        "'{}의 문신.'\n",
        "]"
      ],
      "metadata": {
        "id": "ChST2nVjrnEw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "\n",
        "dataset_name='ffhq'\n",
        "\n",
        "%cd\n",
        "%cd /content/StyleCLIP/global_torch/\n",
        "\n",
        "if not os.path.isfile('./model/'+dataset_name+'.pkl'):\n",
        "        url='https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/'\n",
        "        name='stylegan2-'+dataset_name+'-config-f.pkl'\n",
        "        os.system('wget ' +url+name + '  -P  ./model/')\n",
        "        os.system('mv ./model/'+name+' ./model/'+dataset_name+'.pkl')\n",
        "\n",
        "\n",
        "# input prepare data\n",
        "\n",
        "import clip\n",
        "from manipulate import Manipulator\n",
        "from StyleCLIP import GetDt,GetBoundary\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device,jit=False)\n",
        "\n",
        "network_pkl='./model/'+dataset_name+'.pkl'\n",
        "device = torch.device('cuda')\n",
        "M=Manipulator()\n",
        "M.device=device\n",
        "G=M.LoadModel(network_pkl,device)\n",
        "M.G=G\n",
        "M.SetGParameters()\n",
        "num_img=100_000\n",
        "M.GenerateS(num_img=num_img)\n",
        "M.GetCodeMS()\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "file_path='./npy/'+dataset_name+'/'\n",
        "fs3=np.load(file_path+'fs3.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjlfbVcr2lz",
        "outputId": "8e14133c-88ae-41e1-9619-6f5e6b0ec0de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content/StyleCLIP/global_torch\n",
            "name:conv1_resolution_4 Resolution: 4, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_4 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_8 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_16 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_32 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_64 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_128 Resolution: 128, InC: 512, OutC:256, w_dim: 512\n",
            "name:conv1_resolution_128 Resolution: 128, InC: 256, OutC:256, w_dim: 512\n",
            "name:toRGB_resolution_128 InC: 256, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_256 Resolution: 256, InC: 256, OutC:128, w_dim: 512\n",
            "name:conv1_resolution_256 Resolution: 256, InC: 128, OutC:128, w_dim: 512\n",
            "name:toRGB_resolution_256 InC: 128, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_512 Resolution: 512, InC: 128, OutC:64, w_dim: 512\n",
            "name:conv1_resolution_512 Resolution: 512, InC: 64, OutC:64, w_dim: 512\n",
            "name:toRGB_resolution_512 InC: 64, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_1024 Resolution: 1024, InC: 64, OutC:32, w_dim: 512\n",
            "name:conv1_resolution_1024 Resolution: 1024, InC: 32, OutC:32, w_dim: 512\n",
            "name:toRGB_resolution_1024 InC: 32, OutC:3, w_dim: 512\n",
            "name:conv1_resolution_4 Resolution: 4, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_4 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_8 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_16 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_32 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n",
            "name:conv1_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n",
            "name:toRGB_resolution_64 InC: 512, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_128 Resolution: 128, InC: 512, OutC:256, w_dim: 512\n",
            "name:conv1_resolution_128 Resolution: 128, InC: 256, OutC:256, w_dim: 512\n",
            "name:toRGB_resolution_128 InC: 256, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_256 Resolution: 256, InC: 256, OutC:128, w_dim: 512\n",
            "name:conv1_resolution_256 Resolution: 256, InC: 128, OutC:128, w_dim: 512\n",
            "name:toRGB_resolution_256 InC: 128, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_512 Resolution: 512, InC: 128, OutC:64, w_dim: 512\n",
            "name:conv1_resolution_512 Resolution: 512, InC: 64, OutC:64, w_dim: 512\n",
            "name:toRGB_resolution_512 InC: 64, OutC:3, w_dim: 512\n",
            "name:conv0_resolution_1024 Resolution: 1024, InC: 64, OutC:32, w_dim: 512\n",
            "name:conv1_resolution_1024 Resolution: 1024, InC: 32, OutC:32, w_dim: 512\n",
            "name:toRGB_resolution_1024 InC: 32, OutC:3, w_dim: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "os.chdir('/content/Multilingual-StyleCLIP/models')\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import copy\n",
        "from MultilingualCLIP import multilingual_clip\n",
        "\n",
        "\n",
        "def GetAlign(out, dt, model, preprocess):\n",
        "    imgs = out\n",
        "    imgs1 = imgs.reshape([-1]+list(imgs.shape[2:]))\n",
        "\n",
        "    tmp = []\n",
        "    for i in range(len(imgs1)):\n",
        "\n",
        "        img = Image.fromarray(imgs1[i])\n",
        "        image = preprocess(img).unsqueeze(0).to(device)\n",
        "        tmp.append(image)\n",
        "\n",
        "    image = torch.cat(tmp)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features = image_features / \\\n",
        "            image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    image_features1 = image_features.cpu().numpy()\n",
        "\n",
        "    image_features1 = image_features1.reshape(list(imgs.shape[:2])+[512])\n",
        "\n",
        "    fd = image_features1[:, 1:, :]-image_features1[:, :-1, :]\n",
        "\n",
        "    fd1 = fd.reshape([-1, 512])\n",
        "    fd2 = fd1/np.linalg.norm(fd1, axis=1)[:, None]\n",
        "\n",
        "    tmp = np.dot(fd2, dt)\n",
        "    m = tmp.mean()\n",
        "    acc = np.sum(tmp > 0)/len(tmp)\n",
        "    print(m, acc)\n",
        "    return m, acc\n",
        "\n",
        "\n",
        "def SplitS(ds_p, M, if_std):\n",
        "    all_ds = []\n",
        "    start = 0\n",
        "    for i in M.mindexs:\n",
        "        tmp = M.dlatents[i].shape[1]\n",
        "        end = start+tmp\n",
        "        tmp = ds_p[start:end]\n",
        "\n",
        "        all_ds.append(tmp)\n",
        "        start = end\n",
        "\n",
        "    all_ds2 = []\n",
        "    tmp_index = 0\n",
        "    for i in range(len(M.s_names)):\n",
        "        if (not 'RGB' in M.s_names[i]) and (not len(all_ds[tmp_index]) == 0):\n",
        "\n",
        "            if if_std:\n",
        "                tmp = all_ds[tmp_index]*M.code_std[i]\n",
        "            else:\n",
        "                tmp = all_ds[tmp_index]\n",
        "\n",
        "            all_ds2.append(tmp)\n",
        "            tmp_index += 1\n",
        "        else:\n",
        "            tmp = np.zeros(len(M.dlatents[i][0]))\n",
        "            all_ds2.append(tmp)\n",
        "    return all_ds2\n",
        "\n",
        "## for multilingual  ##\n",
        "\n",
        "def encode_text(txt):\n",
        "    text_model = multilingual_clip.load_model('M-BERT-Base-ViT-B')\n",
        "    return text_model(txt).cuda()\n",
        "\n",
        "def zeroshot_classifier(classnames, templates, model):\n",
        "    with torch.no_grad():\n",
        "        zeroshot_weights = []\n",
        "        for classname in classnames:\n",
        "            texts = [template.format(classname)\n",
        "                     for template in templates]  # format with class\n",
        "\n",
        "            # texts = clip.tokenize(texts).cuda() #tokenize (org)\n",
        "            # class_embeddings = model.encode_text(texts) #embed with text encoder\n",
        "\n",
        "            class_embeddings = encode_text(texts)\n",
        "\n",
        "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "            class_embedding = class_embeddings.mean(dim=0)\n",
        "            class_embedding /= class_embedding.norm()\n",
        "            zeroshot_weights.append(class_embedding)\n",
        "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
        "    return zeroshot_weights\n",
        "\n",
        "\n",
        "def GetDt(classnames, model, templates):\n",
        "    text_features = zeroshot_classifier(classnames, templates, model).t()\n",
        "\n",
        "    dt = text_features[0]-text_features[1]\n",
        "    dt = dt.cpu().numpy()\n",
        "\n",
        "    print(np.linalg.norm(dt))\n",
        "    dt = dt/np.linalg.norm(dt)\n",
        "    return dt\n",
        "\n",
        "########################\n",
        "\n",
        "\n",
        "def GetBoundary(fs3, dt, M, threshold):\n",
        "    tmp = np.dot(fs3, dt)\n",
        "\n",
        "    ds_imp = copy.copy(tmp)\n",
        "    select = np.abs(tmp) < threshold\n",
        "    num_c = np.sum(~select)\n",
        "\n",
        "    ds_imp[select] = 0\n",
        "    tmp = np.abs(ds_imp).max()\n",
        "    ds_imp /= tmp\n",
        "\n",
        "    boundary_tmp2 = SplitS(ds_imp, M, if_std=True)\n",
        "    print('num of channels being manipulated:', num_c)\n",
        "    return boundary_tmp2, num_c\n",
        "\n",
        "\n",
        "def GetFs(file_path):\n",
        "    fs = np.load(file_path+'single_channel.npy')\n",
        "    tmp = np.linalg.norm(fs, axis=-1)\n",
        "    fs1 = fs/tmp[:, :, :, None]\n",
        "    fs2 = fs1[:, :, 1, :]-fs1[:, :, 0, :]  # 5*sigma - (-5)* sigma\n",
        "    fs3 = fs2/np.linalg.norm(fs2, axis=-1)[:, :, None]\n",
        "    fs3 = fs3.mean(axis=1)\n",
        "    fs3 = fs3/np.linalg.norm(fs3, axis=-1)[:, None]\n",
        "    return fs3\n"
      ],
      "metadata": {
        "id": "ybpguPs5q2fF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image_and_get_path(input_image):\n",
        "    if isinstance(input_image, np.ndarray):\n",
        "        input_image = Image.fromarray(input_image) # numpy array to pil\n",
        "\n",
        "    input_image = input_image.convert(\"RGB\")\n",
        "\n",
        "    image_path = '/content/uploaded_image.png'\n",
        "    input_image.save(image_path)\n",
        "\n",
        "    return image_path"
      ],
      "metadata": {
        "id": "8DEzZbcCIlCY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio"
      ],
      "metadata": {
        "id": "YYtuOqCAsMdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from models.global_directions.MapTS import GetFs,GetBoundary,GetDt\n",
        "os.chdir('/content/Multilingual-StyleCLIP')\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def multilingual_global(input_image, neutral, target, beta, alpha):\n",
        "    # Alignment\n",
        "    if isinstance(input_image, np.ndarray):\n",
        "        input_image = Image.fromarray(input_image) # Convert numpy array to PIL Image\n",
        "\n",
        "    input_image = input_image.convert(\"RGB\")\n",
        "\n",
        "    image_path = save_image_and_get_path(input_image)\n",
        "\n",
        "    if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "        !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "        !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "    if experiment_type == \"ffhq_encode\":\n",
        "        aligned_image = run_alignment(image_path)\n",
        "    else:\n",
        "        aligned_image = input_image\n",
        "\n",
        "    aligned_image = aligned_image.resize(resize_dims)\n",
        "\n",
        "    # Inversion\n",
        "    img_transforms = EXPERIMENT_ARGS['transform']\n",
        "    transformed_image = img_transforms(aligned_image)\n",
        "\n",
        "    # Run the image through the model\n",
        "    with torch.no_grad():\n",
        "        images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "        result_image, latent = images[0], latents[0]\n",
        "    torch.save(latents, '/content/latents.pt')\n",
        "\n",
        "    # original image\n",
        "    img_index = 0\n",
        "    latents=torch.load('/content/latents.pt')\n",
        "    dlatents_loaded=M.G.synthesis.W2S(latents)\n",
        "\n",
        "    img_indexs=[img_index]\n",
        "    dlatents_loaded=M.S2List(dlatents_loaded)\n",
        "\n",
        "    dlatent_tmp=[tmp[img_indexs] for tmp in dlatents_loaded]\n",
        "\n",
        "    M.num_images=len(img_indexs)\n",
        "\n",
        "    M.alpha=[0]\n",
        "    M.manipulate_layers=[0]\n",
        "    codes,out=M.EditOneC(0,dlatent_tmp)\n",
        "    original=Image.fromarray(out[0,0]).resize((512,512))\n",
        "    M.manipulate_layers=None\n",
        "\n",
        "\n",
        "    classnames = [target, neutral]\n",
        "    dt = GetDt(classnames, model, imagenet_templates)\n",
        "\n",
        "    M.alpha = [alpha]\n",
        "    boundary, c = GetBoundary(fs3, dt, M, threshold=beta)\n",
        "    codes = M.MSCode(dlatent_tmp, boundary)\n",
        "    out = M.GenerateImg(codes)\n",
        "    output_image = Image.fromarray(out[0,0])\n",
        "\n",
        "    return output_image\n",
        "\n",
        "# Define Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=multilingual_global,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Input Image\"),\n",
        "        gr.Textbox(label=\"Neutral Text\"),\n",
        "        gr.Textbox(label=\"Target Text\"),\n",
        "        gr.Slider(minimum=0.08, maximum=0.3, step=0.01, label=\"Beta\"),\n",
        "        gr.Slider(minimum=-10, maximum=10, step=0.1, label=\"Alpha\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Edited Image\"),\n",
        "    title=\"보정...해줘!\",\n",
        "    css=css\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1M5Vv6YnsG5L",
        "outputId": "63d2cceb-3d46-4f6b-8708-c1e298f519d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0c472d29a85c582df7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0c472d29a85c582df7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1409466\n",
            "num of channels being manipulated: 123\n",
            "0.34881043\n",
            "num of channels being manipulated: 107\n",
            "0.34881043\n",
            "num of channels being manipulated: 107\n",
            "0.1409466\n",
            "num of channels being manipulated: 206\n",
            "0.1409466\n",
            "num of channels being manipulated: 206\n",
            "0.1409466\n",
            "num of channels being manipulated: 63\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 16\n",
            "0.14639546\n",
            "num of channels being manipulated: 109\n",
            "0.14639546\n",
            "num of channels being manipulated: 109\n",
            "0.14639546\n",
            "num of channels being manipulated: 109\n",
            "0.14639546\n",
            "num of channels being manipulated: 109\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 123\n",
            "0.34881043\n",
            "num of channels being manipulated: 65\n",
            "0.07779812\n",
            "num of channels being manipulated: 53\n",
            "0.1409466\n",
            "num of channels being manipulated: 123\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 3\n",
            "0.1409466\n",
            "num of channels being manipulated: 3\n",
            "0.1409466\n",
            "num of channels being manipulated: 123\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1201717\n",
            "num of channels being manipulated: 2\n",
            "0.1201717\n",
            "num of channels being manipulated: 2\n",
            "0.28763044\n",
            "num of channels being manipulated: 3\n",
            "0.28763044\n",
            "num of channels being manipulated: 3\n",
            "0.2134886\n",
            "num of channels being manipulated: 12\n",
            "0.2134886\n",
            "num of channels being manipulated: 2\n",
            "0.2134886\n",
            "num of channels being manipulated: 2\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 33\n",
            "0.1409466\n",
            "num of channels being manipulated: 16\n",
            "0.1409466\n",
            "num of channels being manipulated: 16\n",
            "0.1409466\n",
            "num of channels being manipulated: 8\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0c472d29a85c582df7.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uoC1LpBVQS-D"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}